{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "b833ce70-924f-4b94-b68d-3071e57e8873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "cd2f94ce-7bee-4da4-832f-c11fd33b213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lnr = pd.read_csv('C:\\\\Users\\\\User\\\\Downloads\\\\lr.csv')\n",
    "lgr = pd.read_csv('C:\\\\Users\\\\User\\\\Downloads\\\\lgr.csv')\n",
    "mlr = pd.read_csv('C:\\\\Users\\\\User\\\\Downloads\\\\mlr.csv', index_col = 0)\n",
    "for col in mlr.columns:\n",
    "    mlr[col] = pd.to_numeric(mlr[col], errors='coerce') \n",
    "    # mlr[col] = mlr[col].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "38427eff-5b84-4b06-9bfa-4d8b3816c63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values(df):\n",
    "    # print(df)\n",
    "    return df.fillna(df.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "136f66a8-519a-4063-b684-97a6ed35b389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_duplicates(df):\n",
    "    return df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fb41f5d3-ed38-47de-9522-8ad391193936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers(df):\n",
    "    for x in df.columns:\n",
    "        i = df[x]\n",
    "        q1, q3 = np.percentile(i, 25), np.percentile(i, 75)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound, upper_bound = q1 - (1.5 * iqr), q3 + (1.5 * iqr)\n",
    "        lower_inds, upper_inds = df[i < q1].index, df[i > q3].index\n",
    "        df.loc[lower_inds, x] = lower_bound\n",
    "        df.loc[upper_inds, x] = upper_bound\n",
    "        print(x, lower_bound, upper_bound)\n",
    "    # print(df)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bc275689-6e03-4d31-863b-43887165df51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_norm(df):\n",
    "    for x in df.columns:\n",
    "        i = df[x]\n",
    "        mmin, mmax = min(i), max(i)\n",
    "        df[x] = (i - mmin) / (mmax - mmin)\n",
    "    # print(df)\n",
    "    return df\n",
    "    \n",
    "def zscore_norm(df):\n",
    "    for x in df.columns:\n",
    "        i = df[x]\n",
    "        mn, sd = np.mean(i), np.std(i)\n",
    "        df[x] = (i - mn) / (sd)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "794117cf-cd88-4312-afaa-bcf6eee0777f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.006799</td>\n",
       "      <td>0.225260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.014164</td>\n",
       "      <td>0.455183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.004249</td>\n",
       "      <td>0.186836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.010764</td>\n",
       "      <td>0.358671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.024645</td>\n",
       "      <td>0.808515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>0.016430</td>\n",
       "      <td>0.553936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>0.026344</td>\n",
       "      <td>0.477202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>0.023228</td>\n",
       "      <td>0.820180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>0.018696</td>\n",
       "      <td>0.598773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>0.027478</td>\n",
       "      <td>0.876709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>699 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            x         y\n",
       "0    0.006799  0.225260\n",
       "1    0.014164  0.455183\n",
       "2    0.004249  0.186836\n",
       "3    0.010764  0.358671\n",
       "4    0.024645  0.808515\n",
       "..        ...       ...\n",
       "695  0.016430  0.553936\n",
       "696  0.026344  0.477202\n",
       "697  0.023228  0.820180\n",
       "698  0.018696  0.598773\n",
       "699  0.027478  0.876709\n",
       "\n",
       "[699 rows x 2 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fix(df):\n",
    "    return unit_norm((drop_duplicates(missing_values(df))))\n",
    "\n",
    "linear = fix(lnr)\n",
    "linear\n",
    "# log = fix(lgr)\n",
    "# mlt = fix(mlr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e3aaaf-fc25-4565-ad34-11c835a9acad",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "0c449460-fb13-4b9b-8000-64f3dafced35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at 0  is 2.160729920069691\n",
      "Loss at 1000  is 0.10533606604811963\n",
      "Loss at 2000  is 0.06784626148620158\n",
      "Loss at 3000  is 0.06713133831212235\n",
      "Loss at 4000  is 0.06708683487309257\n",
      "Loss at 5000  is 0.06705477522824396\n",
      "Loss at 6000  is 0.06702316668184143\n",
      "Loss at 7000  is 0.06699178906650494\n",
      "Loss at 8000  is 0.06696063677112223\n",
      "Loss at 9000  is 0.06692970810617044\n",
      "test error:  0.07112675264986949 \n",
      "\n",
      "Loss at 0  is 1.7047515690249184\n",
      "Loss at 1000  is 0.04775083111403396\n",
      "Loss at 2000  is 0.03774532471620144\n",
      "Loss at 3000  is 0.03083668447193768\n",
      "Loss at 4000  is 0.025873242126946683\n",
      "Loss at 5000  is 0.022269013191390255\n",
      "Loss at 6000  is 0.019621433292276412\n",
      "Loss at 7000  is 0.017652154662120865\n",
      "Loss at 8000  is 0.01616746513541129\n",
      "Loss at 9000  is 0.015031667601382899\n",
      "test error:  0.01153786799056568\n"
     ]
    }
   ],
   "source": [
    "class Linear_Regression:\n",
    "    def __init__(self, lr=0.001, iters=10000):\n",
    "        self.iters = iters\n",
    "        self.lr = lr\n",
    "    def train(self, X, Y):\n",
    "        X = np.hstack((X, np.ones((X.shape[0], 1))))\n",
    "        # print(X.shape)\n",
    "        # self.thetas = np.random.randint(low=int(min(X[0])), high=int(max(X[0])+1), size=(X.shape[1],1))\n",
    "        self.thetas = np.random.randn(X.shape[1],1)\n",
    "        gd = 0\n",
    "        for i in range(self.iters):\n",
    "            # print(self.derivative_of_cost(X, Y).shape, self.thetas.shape)\n",
    "            self.thetas -= self.lr * self.derivative_of_cost(X, Y)\n",
    "            if i % 1000 == 0:\n",
    "                print('Loss at', i,' is', self.cost(X,Y))\n",
    "        return self.thetas, self.hyp(X)\n",
    "    def hyp(self,X):\n",
    "        # print(X.shape, self.thetas.shape)\n",
    "        return np.dot(X, self.thetas)\n",
    "    def cost(self,X,Y):\n",
    "        return np.mean(np.square(Y - self.hyp(X)))\n",
    "    def derivative_of_cost(self,X,Y):\n",
    "        error = Y - self.hyp(X)\n",
    "        return -2/X.shape[0] * np.dot(X.T, error)\n",
    "    def test(self, Xt):\n",
    "        return self.hyp(np.hstack((Xt, np.ones((Xt.shape[0], 1)))))\n",
    "        \n",
    "        \n",
    "lr = Linear_Regression()  \n",
    "X, Y = linear['x'].to_numpy().reshape(-1,1), linear['y'].to_numpy().reshape(-1,1)\n",
    "t = int(X.shape[0] * 0.8)\n",
    "params, preds = lr.train(X[:t],Y[:t])\n",
    "preds = lr.test(X[t:])\n",
    "print('test error: ',np.mean(np.square((Y[t:] - preds))), '\\n')\n",
    "\n",
    "lr = Linear_Regression()  \n",
    "mlr = fix(mlr)\n",
    "X, Y = mlr.iloc[:,:-1].to_numpy(), mlr.iloc[:,-1].to_numpy().reshape(-1,1)\n",
    "t = int(X.shape[0] * 0.8)\n",
    "params, preds = lr.train(X[:t],Y[:t])\n",
    "preds = lr.test(X[t:])\n",
    "print('test error: ',np.mean(np.square((Y[t:] - preds))) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "fb40e4e8-d3a3-46e9-a890-1a0183af1200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at 0  is 1.018985813640843\n",
      "Loss at 100  is 0.6168463935490055\n",
      "Loss at 200  is 0.6118790143534504\n",
      "Loss at 300  is 0.6088884073868261\n",
      "Loss at 400  is 0.6067215777684428\n",
      "Loss at 500  is 0.6050841599403899\n",
      "Loss at 600  is 0.6038192864398755\n",
      "Loss at 700  is 0.6028237167235848\n",
      "Loss at 800  is 0.6020263589480412\n",
      "Loss at 900  is 0.6013772708082435\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4238, 23)"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Logistic_Regression:\n",
    "    def __init__(self, lr=0.0001, iters=1000):\n",
    "        self.iters = iters\n",
    "        self.lr = lr\n",
    "    def train(self, X, Y):\n",
    "        X = np.hstack((X, np.ones((X.shape[0],1))))\n",
    "        self.thetas = np.random.randn(X.shape[1], 1)* 0.01 \n",
    "        # print(self.thetas.shape)\n",
    "        for i in range(self.iters):\n",
    "            self.thetas -= self.lr * self.derivative_of_cost(X, Y)\n",
    "            if i %100 == 0:\n",
    "                print('Loss at', i,' is', self.cost(X,Y))\n",
    "        return self.thetas, self.sig(X)\n",
    "    def sig(self, X):\n",
    "        # print('sig',-np.dot(self.thetas.T, X.T))\n",
    "        # return 1 / (1 + np.exp(-np.dot(self.thetas.T, X.T))).T\n",
    "        z = np.dot(X, self.thetas)\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -250, 250))) \n",
    "    def cost(self, X, Y):\n",
    "        preds = self.sig(X)\n",
    "        return -np.mean(Y * np.log2(preds) + (1-Y) * np.log2(1 - preds))\n",
    "    def derivative_of_cost(self, X, Y):\n",
    "        preds = self.sig(X)\n",
    "        # preds = np.clip(preds, 1e-8, 1 - 1e-8) \n",
    "        # print((preds-Y).shape)\n",
    "        # print((np.dot(X.T, preds-Y) /X.shape[0]).shape)\n",
    "        return np.dot(X.T, preds-Y) /X.shape[0]\n",
    "lg = Logistic_Regression()\n",
    "lx = fix(lgr.iloc[:,[1,2,4,10,11,12,13,14]]).to_numpy()\n",
    "ly = missing_values(lgr.iloc[:,-1]).to_numpy().reshape(-1,1)\n",
    "x = np.hstack((lx, missing_values(lgr.iloc[:,:-1])))\n",
    "lg.train(x, ly)\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "8aacbd15-c26c-45f0-b344-35769a7f702c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(18,18))\n",
    "sns.heatmap(lgr.corr(), annot=True, linewidth=0.9)\n",
    "\n",
    "plt.figure(figsize=(18, 18))\n",
    "plt.imshow(lgr.corr())\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "5272a28f-2c4d-4bf7-abeb-6ae815535772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140,) (699, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = np.sort(X)\n",
    "plt.scatter(X[:,0], Y, color='blue', label='Actual Data', alpha=0.6)\n",
    "plt.plot(X[t:,0].reshape(-1,1), preds, color='red', label='Best-fit Line')\n",
    "print(X[t:,0].shape, Y.shape)\n",
    "# Labels and legend\n",
    "plt.xlabel('Feature (X)')\n",
    "plt.ylabel('Target (Y)')\n",
    "plt.title('Scatter Plot with Best-fit Line')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "def predict_3d(x1, x2, theta):\n",
    "    return theta[0] + theta[1] * x1 + theta[2] * x2\n",
    "\n",
    "def plot_3d_scatter_with_plane(x1, x2, y, theta):\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    ax.scatter(x1, x2, y, color='blue', label='Data Points', alpha=0.6)\n",
    "\n",
    "    x1_grid, x2_grid = np.meshgrid(np.linspace(min(x1), max(x1), 20),\n",
    "                                   np.linspace(min(x2), max(x2), 20))\n",
    "    y_pred_grid = predict_3d(x1_grid, x2_grid, theta)\n",
    "\n",
    "    ax.plot_surface(x1_grid, x2_grid, y_pred_grid, color='red', alpha=0.5, label='Regression Plane')\n",
    "\n",
    "    # Axis labels\n",
    "    ax.set_xlabel(\"X1 (Feature 1)\")\n",
    "    ax.set_ylabel(\"X2 (Feature 2)\")\n",
    "    ax.set_zlabel(\"Y (Predictions)\")\n",
    "    ax.set_title(\"3D Scatter Plot with Regression Plane\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "theta = np.array([b0, b1, b2])  \n",
    "plot_3d_scatter_with_plane(x1, x2, ypred, theta)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_best_fit_line(X, Y, theta):\n",
    "    if X.ndim > 1:\n",
    "        X = X.flatten()\n",
    "    \n",
    "    X_sorted = np.sort(X) \n",
    "    print(theta.shape, X_sorted.shape)\n",
    "    Y_pred = []\n",
    "    for x in X_sorted:\n",
    "        Y_pred.append(predict(x, theta))\n",
    "    \n",
    "    plt.scatter(X, Y, color='blue', label='Data Points', alpha=0.6)\n",
    "    \n",
    "    plt.plot(X_sorted, Y_pred, color='red', label='Best Fit Line', linewidth=2)\n",
    "    \n",
    "    plt.xlabel(\"X (Feature)\")\n",
    "    plt.ylabel(\"Y (Target)\")\n",
    "    plt.title(\"Best Fit Line\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "df = pd.read_csv('C:/Users/User/Downloads/data_for_LR.csv')\n",
    "d = df.to_numpy()\n",
    "\n",
    "# plot_best_fit_line(data[:,0], data[:,1], theta)\n",
    "\n",
    "plot_best_fit_line(Xtrain, Ytrain, theta)\n",
    "\n",
    "# plot_best_fit_line(Xtest, Ytest, theta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "a095a6a6-057f-4092-9633-a5fd89d66c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values(df):\n",
    "    # print(df)\n",
    "    return df.fillna(df.mean())\n",
    "def drop_duplicates(df):\n",
    "    return df.drop_duplicates()\n",
    "def outliers(df):\n",
    "    for x in df.columns:\n",
    "        i = df[x]\n",
    "        q1, q3 = np.percentile(i, 25), np.percentile(i, 75)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound, upper_bound = q1 - (1.5 * iqr), q3 + (1.5 * iqr)\n",
    "        lower_inds, upper_inds = df[i < q1].index, df[i > q3].index\n",
    "        df.loc[lower_inds, x] = lower_bound\n",
    "        df.loc[upper_inds, x] = upper_bound\n",
    "        print(x, lower_bound, upper_bound)\n",
    "    # print(df)\n",
    "    return df\n",
    "def unit_norm(df):\n",
    "    for x in df.columns:\n",
    "        i = df[x]\n",
    "        mmin, mmax = min(i), max(i)\n",
    "        df[x] = (i - mmin) / (mmax - mmin)\n",
    "    # print(df)\n",
    "    return df\n",
    "    \n",
    "def zscore_norm(df):\n",
    "    for x in df.columns:\n",
    "        i = df[x]\n",
    "        mn, sd = np.mean(i), np.std(i)\n",
    "        df[x] = (i - mn) / (sd)\n",
    "    return df\n",
    "def fix(df):\n",
    "    return unit_norm((drop_duplicates(missing_values(df))))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(18,18))\n",
    "sns.heatmap(lgr.corr(), annot=True, linewidth=0.9)\n",
    "\n",
    "plt.figure(figsize=(18, 18))\n",
    "plt.imshow(lgr.corr())\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "class Linear_Regression:\n",
    "    def __init__(self, lr=0.001, iters=10000):\n",
    "        self.iters = iters\n",
    "        self.lr = lr\n",
    "    def train(self, X, Y):\n",
    "        X = np.hstack((X, np.ones((X.shape[0], 1))))\n",
    "        # print(X.shape)\n",
    "        # self.thetas = np.random.randint(low=int(min(X[0])), high=int(max(X[0])+1), size=(X.shape[1],1))\n",
    "        self.thetas = np.random.randn(X.shape[1],1)\n",
    "        gd = 0\n",
    "        for i in range(self.iters):\n",
    "            # print(self.derivative_of_cost(X, Y).shape, self.thetas.shape)\n",
    "            self.thetas -= self.lr * self.derivative_of_cost(X, Y)\n",
    "            if i % 1000 == 0:\n",
    "                print('Loss at', i,' is', self.cost(X,Y))\n",
    "        return self.thetas, self.hyp(X)\n",
    "    def hyp(self,X):\n",
    "        # print(X.shape, self.thetas.shape)\n",
    "        return np.dot(X, self.thetas)\n",
    "    def cost(self,X,Y):\n",
    "        return np.mean(np.square(Y - self.hyp(X)))\n",
    "    def derivative_of_cost(self,X,Y):\n",
    "        error = Y - self.hyp(X)\n",
    "        return -2/X.shape[0] * np.dot(X.T, error)\n",
    "    def test(self, Xt):\n",
    "        return self.hyp(np.hstack((Xt, np.ones((Xt.shape[0], 1)))))\n",
    "        \n",
    "        \n",
    "lr = Linear_Regression()  \n",
    "X, Y = linear['x'].to_numpy().reshape(-1,1), linear['y'].to_numpy().reshape(-1,1)\n",
    "t = int(X.shape[0] * 0.8)\n",
    "params, preds = lr.train(X[:t],Y[:t])\n",
    "preds = lr.test(X[t:])\n",
    "print('test error: ',np.mean(np.square((Y[t:] - preds))), '\\n')\n",
    "\n",
    "lr = Linear_Regression()  \n",
    "mlr = fix(mlr)\n",
    "X, Y = mlr.iloc[:,:-1].to_numpy(), mlr.iloc[:,-1].to_numpy().reshape(-1,1)\n",
    "t = int(X.shape[0] * 0.8)\n",
    "params, preds = lr.train(X[:t],Y[:t])\n",
    "preds = lr.test(X[t:])\n",
    "print('test error: ',np.mean(np.square((Y[t:] - preds))) )\n",
    "\n",
    "class Logistic_Regression:\n",
    "    def __init__(self, lr=0.0001, iters=1000):\n",
    "        self.iters = iters\n",
    "        self.lr = lr\n",
    "    def train(self, X, Y):\n",
    "        X = np.hstack((X, np.ones((X.shape[0],1))))\n",
    "        self.thetas = np.random.randn(X.shape[1], 1)* 0.01 \n",
    "        # print(self.thetas.shape)\n",
    "        for i in range(self.iters):\n",
    "            self.thetas -= self.lr * self.derivative_of_cost(X, Y)\n",
    "            if i %100 == 0:\n",
    "                print('Loss at', i,' is', self.cost(X,Y))\n",
    "        return self.thetas, self.sig(X)\n",
    "    def sig(self, X):\n",
    "        # print('sig',-np.dot(self.thetas.T, X.T))\n",
    "        # return 1 / (1 + np.exp(-np.dot(self.thetas.T, X.T))).T\n",
    "        z = np.dot(X, self.thetas)\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -250, 250))) \n",
    "    def cost(self, X, Y):\n",
    "        preds = self.sig(X)\n",
    "        return -np.mean(Y * np.log2(preds) + (1-Y) * np.log2(1 - preds))\n",
    "    def derivative_of_cost(self, X, Y):\n",
    "        preds = self.sig(X)\n",
    "        # preds = np.clip(preds, 1e-8, 1 - 1e-8) \n",
    "        # print((preds-Y).shape)\n",
    "        # print((np.dot(X.T, preds-Y) /X.shape[0]).shape)\n",
    "        return np.dot(X.T, preds-Y) /X.shape[0]\n",
    "lg = Logistic_Regression()\n",
    "lx = fix(lgr.iloc[:,[1,2,4,10,11,12,13,14]]).to_numpy()\n",
    "ly = missing_values(lgr.iloc[:,-1]).to_numpy().reshape(-1,1)\n",
    "x = np.hstack((lx, missing_values(lgr.iloc[:,:-1])))\n",
    "lg.train(x, ly)\n",
    "x.shape\n",
    "\n",
    "s = np.sort(X)\n",
    "plt.scatter(X[:,0], Y, color='blue', label='Actual Data', alpha=0.6)\n",
    "plt.plot(X[t:,0].reshape(-1,1), preds, color='red', label='Best-fit Line')\n",
    "print(X[t:,0].shape, Y.shape)\n",
    "# Labels and legend\n",
    "plt.xlabel('Feature (X)')\n",
    "plt.ylabel('Target (Y)')\n",
    "plt.title('Scatter Plot with Best-fit Line')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "def predict_3d(x1, x2, theta):\n",
    "    return theta[0] + theta[1] * x1 + theta[2] * x2\n",
    "\n",
    "def plot_3d_scatter_with_plane(x1, x2, y, theta):\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    ax.scatter(x1, x2, y, color='blue', label='Data Points', alpha=0.6)\n",
    "\n",
    "    x1_grid, x2_grid = np.meshgrid(np.linspace(min(x1), max(x1), 20),\n",
    "                                   np.linspace(min(x2), max(x2), 20))\n",
    "    y_pred_grid = predict_3d(x1_grid, x2_grid, theta)\n",
    "\n",
    "    ax.plot_surface(x1_grid, x2_grid, y_pred_grid, color='red', alpha=0.5, label='Regression Plane')\n",
    "\n",
    "    # Axis labels\n",
    "    ax.set_xlabel(\"X1 (Feature 1)\")\n",
    "    ax.set_ylabel(\"X2 (Feature 2)\")\n",
    "    ax.set_zlabel(\"Y (Predictions)\")\n",
    "    ax.set_title(\"3D Scatter Plot with Regression Plane\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "theta = np.array([b0, b1, b2])  \n",
    "plot_3d_scatter_with_plane(x1, x2, ypred, theta)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_best_fit_line(X, Y, theta):\n",
    "    if X.ndim > 1:\n",
    "        X = X.flatten()\n",
    "    \n",
    "    X_sorted = np.sort(X) \n",
    "    print(theta.shape, X_sorted.shape)\n",
    "    Y_pred = []\n",
    "    for x in X_sorted:\n",
    "        Y_pred.append(predict(x, theta))\n",
    "    \n",
    "    plt.scatter(X, Y, color='blue', label='Data Points', alpha=0.6)\n",
    "    \n",
    "    plt.plot(X_sorted, Y_pred, color='red', label='Best Fit Line', linewidth=2)\n",
    "    \n",
    "    plt.xlabel(\"X (Feature)\")\n",
    "    plt.ylabel(\"Y (Target)\")\n",
    "    plt.title(\"Best Fit Line\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "df = pd.read_csv('C:/Users/User/Downloads/data_for_LR.csv')\n",
    "d = df.to_numpy()\n",
    "\n",
    "# plot_best_fit_line(data[:,0], data[:,1], theta)\n",
    "\n",
    "plot_best_fit_line(Xtrain, Ytrain, theta)\n",
    "\n",
    "# plot_best_fit_line(Xtest, Ytest, theta)\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.tree = None\n",
    "\n",
    "    def calculate_splits(self, feature_column):\n",
    "        sorted_feature = np.sort(feature_column)\n",
    "        splits = [(sorted_feature[i] + sorted_feature[i+1]) / 2 for i in range(len(sorted_feature)-1)]\n",
    "        return splits\n",
    "\n",
    "    def calculate_entropy(self, y):\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / counts.sum()\n",
    "        entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "        return entropy\n",
    "\n",
    "    def calculate_information_gain(self, y, left_y, right_y):\n",
    "        parent_entropy = self.calculate_entropy(y)\n",
    "        left_weight = len(left_y) / len(y)\n",
    "        right_weight = len(right_y) / len(y)\n",
    "        weighted_entropy = left_weight * self.calculate_entropy(left_y) + right_weight * self.calculate_entropy(right_y)\n",
    "        info_gain = parent_entropy - weighted_entropy\n",
    "        return info_gain\n",
    "\n",
    "    def calculate_best_split(self, X, y, feature_index, is_categorical):\n",
    "        feature_column = X[:, feature_index]\n",
    "        if is_categorical:\n",
    "            unique_values = np.unique(feature_column)\n",
    "            best_gain = -1\n",
    "            best_split = None\n",
    "            for value in unique_values:\n",
    "                left_indices = feature_column == value\n",
    "                right_indices = feature_column != value\n",
    "                left_y, right_y = y[left_indices], y[right_indices]\n",
    "                if len(left_y) > 0 and len(right_y) > 0:\n",
    "                    gain = self.calculate_information_gain(y, left_y, right_y)\n",
    "                    if gain > best_gain:\n",
    "                        best_gain = gain\n",
    "                        best_split = value\n",
    "            return best_split, best_gain\n",
    "        else:\n",
    "            possible_splits = self.calculate_splits(feature_column)\n",
    "            best_gain = -1\n",
    "            best_split = None\n",
    "            for split_value in possible_splits:\n",
    "                left_indices = feature_column < split_value\n",
    "                right_indices = feature_column >= split_value\n",
    "                left_y, right_y = y[left_indices], y[right_indices]\n",
    "                if len(left_y) > 0 and len(right_y) > 0:\n",
    "                    gain = self.calculate_information_gain(y, left_y, right_y)\n",
    "                    if gain > best_gain:\n",
    "                        best_gain = gain\n",
    "                        best_split = split_value\n",
    "            return best_split, best_gain\n",
    "\n",
    "    def fit(self, X, y, depth=0, feature_types=None):\n",
    "        num_samples, num_features = X.shape\n",
    "        if num_samples >= self.min_samples_split and depth < self.max_depth:\n",
    "            best_gain = -1\n",
    "            b_split = None\n",
    "\n",
    "            for feature_index in range(num_features):\n",
    "                is_categorical = feature_types[feature_index] == \"categorical\"\n",
    "                split_value, gain = self.calculate_best_split(X, y, feature_index, is_categorical)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    b_split = {'feature_index': feature_index, 'split_value': split_value, 'is_categorical': is_categorical}\n",
    "\n",
    "            if b_split is not None:\n",
    "                feature_column = X[:, b_split['feature_index']]\n",
    "                if b_split['is_categorical']:\n",
    "                    left_indices = feature_column == b_split['split_value']\n",
    "                else:\n",
    "                    left_indices = feature_column < b_split['split_value']\n",
    "                right_indices = ~left_indices\n",
    "\n",
    "                left_subtree = self.fit(X[left_indices], y[left_indices], depth + 1, feature_types)\n",
    "                right_subtree = self.fit(X[right_indices], y[right_indices], depth + 1, feature_types)\n",
    "                return {\n",
    "                    'feature': b_split['feature_index'], \n",
    "                    'split_value': b_split['split_value'], \n",
    "                    'is_categorical': b_split['is_categorical'],\n",
    "                    'left': left_subtree, \n",
    "                    'right': right_subtree\n",
    "                }\n",
    "\n",
    "        return np.bincount(y).argmax()\n",
    "\n",
    "    def fit_with_types(self, X, y, feature_types):\n",
    "        self.tree = self.fit(X, y, feature_types=feature_types)\n",
    "        return self.tree\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self.calculate_prediction(sample, self.tree) for sample in X])\n",
    "    def predict_single(self, sample):\n",
    "        return self.calculate_prediction(sample, self.tree)\n",
    "\n",
    "    def calculate_prediction(self, sample, tree):\n",
    "        if isinstance(tree, dict):\n",
    "            feature_value = sample[tree['feature']]\n",
    "            if tree['is_categorical']:\n",
    "                branch = tree['left'] if feature_value == tree['split_value'] else tree['right']\n",
    "            else:\n",
    "                branch = tree['left'] if feature_value < tree['split_value'] else tree['right']\n",
    "            return self.calculate_prediction(sample, branch)\n",
    "        return tree\n",
    "    def print_tree(self, tree=None, depth=0):\n",
    "        if tree is None:\n",
    "            tree = self.tree\n",
    "        if isinstance(tree, dict):\n",
    "            feature = tree['feature']\n",
    "            split_value = tree['split_value']\n",
    "            split_type = \"==\" if tree['is_categorical'] else \"<\"\n",
    "            print(f\"{'|  ' * depth}Feature {feature} {split_type} {split_value}\")\n",
    "            print(f\"{'|  ' * depth}--> Left:\")\n",
    "            self.print_tree(tree['left'], depth + 1)\n",
    "            print(f\"{'|  ' * depth}--> Right:\")\n",
    "            self.print_tree(tree['right'], depth + 1)\n",
    "        else:\n",
    "            print(f\"{'|  ' * depth}Prediction: {tree}\")\n",
    "\n",
    "# Load and preprocess the iris dataset\n",
    "data = pd.read_csv(\"./iris.csv\", index_col=0)\n",
    "X = data.drop(columns=\"Species\").values\n",
    "Y = data[\"Species\"].values\n",
    "classes, Y = np.unique(Y, return_inverse=True)\n",
    "\n",
    "# Split into train and test sets\n",
    "def train_test_split(X, Y, test_size=0.2):\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(X.shape[0] * (1 - test_size))\n",
    "    train_indices, test_indices = indices[:split], indices[split:]\n",
    "    return X[train_indices], X[test_indices], Y[train_indices], Y[test_indices]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "print(X_train.shape)\n",
    "# Train the DecisionTree model\n",
    "feature_types = [\"numerical\", \"numerical\", \"numerical\", \"numerical\"]\n",
    "tree = DecisionTree(max_depth=3)\n",
    "tree.fit_with_types(X_train, Y_train, feature_types)\n",
    "\n",
    "print(\"\\nDecision Tree Structure:\")\n",
    "tree.print_tree()\n",
    "\n",
    "# Make individual predictions\n",
    "sample_index = 0  # Example: Predict the first test sample\n",
    "sample_prediction = tree.predict_single(X_test[sample_index])\n",
    "print(f\"\\nPrediction for sample {sample_index}: {classes[sample_prediction]} (Actual: {classes[Y_test[sample_index]]})\")\n",
    "\n",
    "\n",
    "# Make predictions and calculate accuracy\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.sum(y_true == y_pred) / len(y_true)\n",
    "\n",
    "Y_pred = tree.predict(X_test)\n",
    "acc = accuracy(Y_test, Y_pred)\n",
    "\n",
    "# Generate confusion matrix\n",
    "def confusion_matrix(y_true, y_pred, num_classes):\n",
    "    matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
    "    for t, p in zip(y_true, y_pred):\n",
    "        matrix[t, p] += 1\n",
    "    return matrix\n",
    "\n",
    "conf_mat = confusion_matrix(Y_test, Y_pred, num_classes=len(classes))\n",
    "\n",
    "# Results\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_mat)\n",
    "print(tree.tree)\n",
    "data\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, n_estimators=10, max_depth=None, min_samples_split=2, min_samples_leaf=1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.trees = []\n",
    "\n",
    "    def generate_samples(self, X, Y):\n",
    "        n_samples = X.shape[0]\n",
    "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        return X[indices], Y[indices]\n",
    "\n",
    "    def fit(self, X, Y, feature_types):\n",
    "        self.trees = []\n",
    "        for i in range(self.n_estimators):\n",
    "            X_sample, y_sample = self.generate_samples(X, Y)\n",
    "            tree = DecisionTree(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                min_samples_leaf=self.min_samples_leaf\n",
    "            )\n",
    "            tree.fit_with_types(X_sample, y_sample, feature_types)\n",
    "            self.trees.append(tree)\n",
    "            print(f\"Tree {i + 1}:\")\n",
    "            self.print_tree_structure(tree.tree)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        tree_predictions = np.array([tree.predict(X_test) for tree in self.trees])\n",
    "        tree_predictions = tree_predictions.T  # Shape: (num_samples, num_trees)\n",
    "        return np.array([self.majority_vote(preds) for preds in tree_predictions])\n",
    "\n",
    "    def majority_vote(self, predictions):\n",
    "        unique, counts = np.unique(predictions, return_counts=True)\n",
    "        return unique[np.argmax(counts)]\n",
    "\n",
    "    def confusion_matrix(self, y_true, y_pred):\n",
    "        unique_labels = np.unique(np.concatenate((y_true, y_pred)))\n",
    "        num_classes = len(unique_labels)\n",
    "        matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
    "\n",
    "        for true, pred in zip(y_true, y_pred):\n",
    "            matrix[true, pred] += 1\n",
    "\n",
    "        accuracy = np.trace(matrix) / np.sum(matrix)\n",
    "        return matrix, accuracy\n",
    "\n",
    "    def print_tree_structure(self, tree, depth=0):\n",
    "        if isinstance(tree, dict):\n",
    "            feature = tree['feature']\n",
    "            split_value = tree['split_value']\n",
    "            is_categorical = tree['is_categorical']\n",
    "            indent = \" \" * (depth * 4)\n",
    "            print(f\"{indent}Feature {feature}, Split Value: {split_value}, Is Categorical: {is_categorical}\")\n",
    "            self.print_tree_structure(tree['left'], depth + 1)\n",
    "            self.print_tree_structure(tree['right'], depth + 1)\n",
    "        else:\n",
    "            indent = \" \" * (depth * 4)\n",
    "            print(f\"{indent}Leaf: {tree}\")\n",
    "\n",
    "# Load and preprocess the iris dataset\n",
    "data = pd.read_csv(\"./iris.csv\", index_col=0)\n",
    "X = data.drop(columns=\"Species\").values\n",
    "Y = data[\"Species\"].values\n",
    "classes, Y = np.unique(Y, return_inverse=True)\n",
    "\n",
    "# Split into train and test sets\n",
    "def train_test_split(X, Y, test_size=0.2):\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(X.shape[0] * (1 - test_size))\n",
    "    train_indices, test_indices = indices[:split], indices[split:]\n",
    "    return X[train_indices], X[test_indices], Y[train_indices], Y[test_indices]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "# Train the RandomForest model\n",
    "feature_types = [\"numerical\", \"numerical\", \"numerical\", \"numerical\"]\n",
    "rf = RandomForest(n_estimators=5, max_depth=3)\n",
    "rf.fit(X_train, Y_train, feature_types)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "Y_pred = rf.predict(X_test)\n",
    "conf_matrix, acc = rf.confusion_matrix(Y_test, Y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Accuracy:\", acc)\n",
    "\n",
    "def manual_one_hot_encoding(data, categories=None):\n",
    "    if categories is None:\n",
    "        categories = np.unique(data)\n",
    "    category_mapping = {category: idx for idx, category in enumerate(categories)}\n",
    "    encoded_array = np.zeros((len(data), len(categories)), dtype=int)\n",
    "    for row_idx, value in enumerate(data):\n",
    "        col_idx = category_mapping[value]\n",
    "        encoded_array[row_idx, col_idx] = 1\n",
    "    return encoded_array, category_mapping\n",
    "\n",
    "data = [\"cat\", \"dog\", \"bird\", \"cat\", \"dog\", \"cat\"]\n",
    "\n",
    "one_hot_encoded, mapping = manual_one_hot_encoding(data)\n",
    "\n",
    "print(\"Original Data:\", data)\n",
    "print(\"Category Mapping:\", mapping)\n",
    "print(\"One-Hot Encoded Matrix:\\n\", one_hot_encoded)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PCA with graphs\n",
    "def pca(data, n_components=2):\n",
    "    mean = np.mean(data, axis=0)\n",
    "    centered_data = data - mean\n",
    "    cov_matrix = np.cov(centered_data, rowvar=False)\n",
    "    eig_values, eig_vectors = np.linalg.eig(cov_matrix)\n",
    "    indices = np.argsort(eig_values)[::-1]\n",
    "    eig_vectors = eig_vectors[:, indices][:, :n_components]\n",
    "    reduced_data = np.dot(centered_data, eig_vectors)\n",
    "\n",
    "    explained = eig_values[indices][:n_components] / np.sum(eig_values)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(range(len(explained)), explained)\n",
    "    plt.title('Explained Variance')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.scatter(reduced_data[:, 0], reduced_data[:, 1])\n",
    "    plt.title('PCA Scatter')\n",
    "    plt.show()\n",
    "\n",
    "    return reduced_data\n",
    "\n",
    "# Naive Bayes class\n",
    "class NB:\n",
    "    def fit(self, x, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.class_priors = {cls: np.mean(y == cls) for cls in self.classes}\n",
    "        self.feature_params = {}\n",
    "        for cls in self.classes:\n",
    "            cls_data = x[y == cls]\n",
    "            self.feature_params[cls] = {\n",
    "                'mean': np.mean(cls_data, axis=0),\n",
    "                'var': np.var(cls_data, axis=0)\n",
    "            }\n",
    "\n",
    "    def predict_one(self, x):\n",
    "        likelihoods = []\n",
    "        for cls in self.classes:\n",
    "            mean = self.feature_params[cls]['mean']\n",
    "            var = self.feature_params[cls]['var']\n",
    "            prior = self.class_priors[cls]\n",
    "            likelihood = np.sum(-0.5 * np.log(2 * np.pi * var) - ((x - mean) ** 2) / (2 * var)) + np.log(prior)\n",
    "            likelihoods.append(likelihood)\n",
    "        return self.classes[np.argmax(likelihoods)]\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.array([self.predict_one(sample) for sample in x])\n",
    "\n",
    "    def score(self, x, y):\n",
    "        preds = self.predict(x)\n",
    "        return np.mean(preds == y)\n",
    "\n",
    "# Gaussian Bayes (Multivariate)\n",
    "class GaussianBayes:\n",
    "    def __init__(self):\n",
    "        self.means = {}\n",
    "        self.covmats = {}\n",
    "        self.priors = {}\n",
    "        self.classes = None\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        self.classes = np.unique(Y)\n",
    "        rows, cols = X.shape\n",
    "        self.means = {i: np.zeros(cols) for i in self.classes}\n",
    "        self.covmats = {i: np.zeros((cols, cols)) for i in self.classes}\n",
    "        self.priors = {i: 0 for i in self.classes}\n",
    "\n",
    "        for c in self.classes:\n",
    "            self.means[c] = np.mean(X[Y == c], axis=0)\n",
    "            self.priors[c] = (X[Y == c]).shape[0] / rows\n",
    "            self.covmats[c] = np.cov(X[Y == c].T)\n",
    "\n",
    "    def test(self, X):\n",
    "        pclasses = []\n",
    "        probability = []\n",
    "        for i in range(X.shape[0]):\n",
    "            probs_per_example = {k: 0 for k in self.classes}\n",
    "            for c in self.classes:\n",
    "                diff = X[i] - self.means[c]\n",
    "                denum = ((2 * np.pi) ** (X.shape[1] / 2)) * np.sqrt(np.linalg.det(self.covmats[c]))\n",
    "                num = -0.5 * (diff.T @ np.linalg.inv(self.covmats[c]) @ diff)\n",
    "                pxc = np.exp(num) / denum\n",
    "                probs_per_example[c] = self.priors[c] * pxc\n",
    "            pclasses.append(max(probs_per_example, key=lambda x: probs_per_example[x]))\n",
    "            probability.append(max(probs_per_example.values()))\n",
    "\n",
    "        return pclasses, probability\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.test(X)[0]\n",
    "\n",
    "# Naive Bayes for Text Classification\n",
    "class NaiveBayesText:\n",
    "    def __init__(self, classes):\n",
    "        self.classes = classes\n",
    "        self.class_bow = []\n",
    "        self.class_prob = []\n",
    "        self.class_total = []\n",
    "\n",
    "    def addExample(self, x, y):\n",
    "        pass  # To be implemented\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        rows, cols = X.shape\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            self.class_bow.append(get_BagofWords(X[Y == c].flatten()))\n",
    "            self.class_prob.append(len(X[Y == c]) / rows)\n",
    "            sum_freqs = 0\n",
    "            for k, v in self.class_bow[idx].items():\n",
    "                sum_freqs += v\n",
    "            self.class_total.append(len(X[Y == c]) + sum_freqs)\n",
    "\n",
    "    def test(self, X):\n",
    "        nexamples, nfeatures = X.shape\n",
    "        pclass = []\n",
    "        for i in range(X.shape[0]):\n",
    "            fprobs = {k: 0 for k in self.classes}\n",
    "            x_words = parse_string(X[i])\n",
    "            for idx, c in enumerate(self.classes):\n",
    "                pxcs = []\n",
    "                for j in x_words:\n",
    "                    if j in self.class_bow[idx]:\n",
    "                        pxc = self.class_bow[idx][j] + 1\n",
    "                    else:\n",
    "                        pxc = 1\n",
    "                    pxcs.append(pxc / self.class_total[idx])\n",
    "                log_fx = sum(np.log(p) for p in pxcs)\n",
    "                fx = log_fx\n",
    "                fprobs[c] = fx * self.class_prob[idx]\n",
    "            pclass.append(max(fprobs, key=lambda x: fprobs[x]))\n",
    "\n",
    "        return np.array(pclass)\n",
    "\n",
    "    def predict(self, x):\n",
    "        x_array = np.array(x)\n",
    "        if x_array.ndim == 1:\n",
    "            x_array = x_array.reshape([-1, 1])\n",
    "\n",
    "        return self.test(x_array)[0]\n",
    "\n",
    "# Example data\n",
    "x = np.random.rand(100, 5)  # Random data for PCA and Naive Bayes\n",
    "y = np.random.randint(0, 2, 100)  # Binary labels\n",
    "\n",
    "# Run PCA\n",
    "reduced = pca(x)\n",
    "\n",
    "# Naive Bayes\n",
    "nb = NB()\n",
    "nb.fit(x, y)\n",
    "single_pred_nb = nb.predict_one(x[0])\n",
    "all_preds_nb = nb.predict(x)\n",
    "acc_nb = nb.score(x, y)\n",
    "\n",
    "# Output for PCA\n",
    "print(\"Reduced data (PCA):\")\n",
    "print(reduced)\n",
    "\n",
    "# Output for Naive Bayes\n",
    "print(\"\\nNaive Bayes:\")\n",
    "print(\"Single prediction:\", single_pred_nb)\n",
    "print(\"All predictions:\", all_preds_nb)\n",
    "print(\"Accuracy:\", acc_nb)\n",
    "\n",
    "# Gaussian Bayes (Multivariate)\n",
    "bayes = GaussianBayes()\n",
    "bayes.train(x, y)\n",
    "single_pred_bayes = bayes.predict(x[0].reshape(1, -1))\n",
    "all_preds_bayes = bayes.predict(x)\n",
    "\n",
    "print(\"\\nGaussian Bayes:\")\n",
    "print(\"Single prediction:\", single_pred_bayes)\n",
    "print(\"All predictions:\", all_preds_bayes)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def standardize(X):\n",
    "    \"\"\"Standardize the input data by subtracting the mean and dividing by the standard deviation.\"\"\"\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std_dev = np.std(X, axis=0)\n",
    "    X_standardized = (X - mean) / std_dev\n",
    "    return X_standardized\n",
    "\n",
    "def compute_covariance_matrix(X):\n",
    "    \"\"\"Compute the covariance matrix of the input data.\"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    covariance_matrix = np.dot(X.T, X) / (n_samples - 1)\n",
    "    return covariance_matrix\n",
    "\n",
    "def eigen_decomposition(cov_matrix):\n",
    "    \"\"\"Compute eigenvalues and eigenvectors from the covariance matrix.\"\"\"\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "    return eigenvalues, eigenvectors\n",
    "\n",
    "def project_data(X, eigenvectors, n_components):\n",
    "    \"\"\"Project the data onto the first n_components eigenvectors (principal components).\"\"\"\n",
    "    W = eigenvectors[:, :n_components] \n",
    "    X_pca = np.dot(X, W)\n",
    "    return X_pca\n",
    "\n",
    "# Example for any general dataset (e.g., df is your DataFrame)\n",
    "# Assuming 'df' is your dataframe and you want to drop the 'target_column' for features and keep 'target_column' as labels\n",
    "# X = df.drop(columns='target_column').values  # Convert to numpy array (features)\n",
    "# Y = df['target_column'].values  # Target labels (class labels)\n",
    "from sklearn.datasets import make_classification\n",
    "X, Y = make_classification(n_samples=200, n_features=5, n_informative=3, n_clusters_per_class=2, random_state=42)\n",
    "\n",
    "\n",
    "# Standardize the data\n",
    "X_standardized = standardize(X)\n",
    "\n",
    "# Compute covariance matrix and perform eigen decomposition\n",
    "cov_matrix = compute_covariance_matrix(X_standardized)\n",
    "eigenvalues, eigenvectors = eigen_decomposition(cov_matrix)\n",
    "\n",
    "# Calculate explained variance ratio\n",
    "explained_variance_ratio = eigenvalues / np.sum(eigenvalues)\n",
    "print(\"Explained variance by each component: {}\".format(explained_variance_ratio))\n",
    "\n",
    "# Project the data onto 2 principal components\n",
    "X_pca = project_data(X_standardized, eigenvectors, n_components=2)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=Y, cmap='viridis', edgecolor='k')\n",
    "plt.colorbar(scatter, label='Class Label')\n",
    "\n",
    "# Add axis labels showing the variance explained by each principal component\n",
    "plt.xlabel(f'Principal Component 1 ({explained_variance_ratio[0] * 100:.2f}% Variance)')\n",
    "plt.ylabel(f'Principal Component 2 ({explained_variance_ratio[1] * 100:.2f}% Variance)')\n",
    "plt.title('PCA on General Dataset (2 Components)')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# PROJECTIONS\n",
    "# can freely choose values of a and b, and find value of c to satisfy equation v1.v2 = 0\n",
    "a = 1  \n",
    "b = 0  \n",
    "x = v1[0] * a\n",
    "y = v1[1] * b\n",
    "c = -(x + y) / v1[2]\n",
    "v2 = np.array([a, b, c])\n",
    "print(np.dot(v1,v2))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.quiver(0, 0, 0, v1[0], v1[1], v1[2], color='r', label='v1', length=4.0, normalize=True)\n",
    "ax.quiver(0, 0, 0, v2[0], v2[1], v2[2], color='b', label='v2', length=4.0, normalize=True)\n",
    "ax.set_xlim([-1, 5])\n",
    "ax.set_ylim([-1, 10])\n",
    "ax.set_zlim([-1, 8])\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "# ORTHOGNALS\n",
    "v1ov2 = (np.dot(v1, v2)/np.dot(v2, v2))*v2\n",
    "v2ov1 = (np.dot(v2, v1)/np.dot(v1, v1))*v1\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.quiver(0, 0, 0, v1[0], v1[1], v1[2], color='r', label='v1', length=3.0, normalize=True)\n",
    "ax.quiver(0, 0, 0, v2[0], v2[1], v2[2], color='b', label='v2', length=3.0, normalize=True)\n",
    "ax.quiver(0, 0, 0, v1ov2[0], v1ov2[1], v1ov2[2], color='g', label='proj v1 on v2', length=1.0, normalize=True)\n",
    "ax.quiver(0, 0, 0, v2ov1[0], v2ov1[1], v2ov1[2], color='m', label='proj v2 on v1', length=1.0, normalize=True)\n",
    "ax.set_xlim([-1, 5])\n",
    "ax.set_ylim([-1, 10])\n",
    "ax.set_zlim([-1, 8])\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "# RECONSTRUCTEDSS\n",
    "og_v1 = v1ov2 + (v1 - v1ov2)  \n",
    "og_v2 = v2ov1 + (v2 - v2ov1)  \n",
    "\n",
    "print(\"Original v1:\", v1)\n",
    "print(\"Reconstructed v1:\", og_v1)\n",
    "\n",
    "print(\"Original v2:\", v2)\n",
    "print(\"Reconstructed v2:\", og_v2)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.quiver(0, 0, 0, v1[0], v1[1], v1[2], color='r', label='Original v1', length=4.0, normalize=True)\n",
    "ax.quiver(0, 0, 0, v2[0], v2[1], v2[2], color='b', label='Original v2', length=4.0, normalize=True)\n",
    "ax.quiver(0, 0, 0, og_v1[0], og_v1[1], og_v1[2], color='g', linestyle='dashed', label='Reconstructed v1', length=4.0, normalize=True)\n",
    "ax.quiver(0, 0, 0, og_v2[0], og_v2[1], og_v2[2], color='m', linestyle='dashed', label='Reconstructed v2', length=4.0, normalize=True)\n",
    "ax.set_xlim([-1, 5])\n",
    "ax.set_ylim([-1, 10])\n",
    "ax.set_zlim([-1, 8])\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "np.random.seed(42)\n",
    "v3 = np.random.uniform(low=1, high=10, size=3)\n",
    "v4 = np.random.uniform(low=1, high=10, size=3)\n",
    "\n",
    "v3ov4 = (np.dot(v3, v4)/np.dot(v4, v4))*v4\n",
    "v4ov3 = (np.dot(v4, v3)/np.dot(v3, v3))*v3\n",
    "perp_v3 = v3- v3ov4\n",
    "perp_v4 = v4- v4ov3\n",
    "\n",
    "og_v3 = v3ov4 + perp_v3\n",
    "og_v4 = v4ov3 + perp_v4\n",
    "\n",
    "# Print the results\n",
    "print(\"Original v3:\", v3)\n",
    "print(\"Original v4:\", v4)\n",
    "print(\"Projection of v3 on v4 (v3ov4):\", v3ov4)\n",
    "print(\"Projection of v4 on v3 (v4ov3):\", v4ov3)\n",
    "print(\"Reconstructed v3 (og_v3):\", og_v3)\n",
    "print(\"Reconstructed v4 (og_v4):\", og_v4)\n",
    "# Take two vectors and project one on the other, and find the initial vectors from the projections.\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.quiver(0, 0, 0, v3[0], v3[1], v3[2], color='r', label='Original v3', length=10.0, normalize=True)\n",
    "ax.quiver(0, 0, 0, v4[0], v4[1], v4[2], color='b', label='Original v4', length=10.0, normalize=True)\n",
    "ax.quiver(0, 0, 0, v3ov4[0], v3ov4[1], v3ov4[2], color='g', linestyle='dashed', label='v3ov4', length=14.0, normalize=True)\n",
    "ax.quiver(0, 0, 0, v4ov3[0], v4ov3[1], v4ov3[2], color='m', linestyle='dashed', label='v4ov3', length=14.0, normalize=True)\n",
    "ax.set_xlim([-10, 10])\n",
    "ax.set_ylim([-10, 10])\n",
    "ax.set_zlim([-10, 10])\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "# ZERO CENTER PLOT\n",
    "df = pd.read_csv('/kaggle/input/iris/Iris.csv')\n",
    "slm = df['SepalLengthCm'].mean()\n",
    "swm = df['SepalWidthCm'].mean()\n",
    "plm = df['PetalLengthCm'].mean()\n",
    "pwm = df['PetalWidthCm'].mean()\n",
    "\n",
    "rows = df.shape[0]\n",
    "mean = np.array([slm, swm, plm, pwm]).reshape(-1,1)\n",
    "mean = np.dot(np.ones((1,rows)).T, mean.T)\n",
    "df_new = df[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']].to_numpy()\n",
    "z = df_new - mean\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "feature_names = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\n",
    "\n",
    "for i in range(4):\n",
    "    axes[i].hist(z[:, i], bins=20, edgecolor='k', alpha=0.7)\n",
    "    axes[i].set_title(f'Deviation of {feature_names[i]}')\n",
    "    axes[i].set_xlabel('Deviation')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# MEAN WRT BEFORE ZERO CENTER:\n",
    "features = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\n",
    "means = [slm, swm, plm, pwm]\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    axes[i].hist(df[feature], bins=20, edgecolor='k', alpha=0.7)\n",
    "    axes[i].axvline(means[i], color='r', linestyle='--', label='Mean')\n",
    "    axes[i].set_title(f'Distribution of {feature}')\n",
    "    axes[i].set_xlabel(feature)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# AFTER ZER CENRING:\n",
    "features = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    centered_feature = z[:, i]\n",
    "    axes[i].hist(centered_feature, bins=20, edgecolor='k', alpha=0.7)\n",
    "    axes[i].axvline(0, color='r', linestyle='--', label='Mean (0)')\n",
    "    axes[i].set_title(f'Distribution of {feature} After Zero-Centering')\n",
    "    axes[i].set_xlabel(feature)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# MAHAL DIST\n",
    "mahal = []\n",
    "for i in range(len(sda)-1):\n",
    "    diff = sda[i] - sda[i+1]\n",
    "    diff = diff.reshape([-1,1])\n",
    "#     print(diff.shape, inv_cov_mat.shape, diff.T.shape)\n",
    "    m = np.dot(diff.T, inv_cov_mat)\n",
    "    m = np.dot(m, diff)\n",
    "    mahal.append(m)\n",
    "mahal\n",
    "\n",
    "# KFOLD\n",
    "def k_fold_cross_validation(X, y, k=5):\n",
    "    fold_size = len(X) // k\n",
    "    accuracies = []\n",
    "    \n",
    "    for fold in range(k):\n",
    "        # Split data into train and test sets for the current fold\n",
    "        test_indices = list(range(fold * fold_size, (fold + 1) * fold_size))\n",
    "        train_indices = list(set(range(len(X))) - set(test_indices))\n",
    "        \n",
    "        X_train, y_train = X[train_indices], y[train_indices]\n",
    "        X_test, y_test = X[test_indices], y[test_indices]\n",
    "        \n",
    "        # Train model\n",
    "        weights = train_logistic_regression(X_train, y_train)\n",
    "        \n",
    "        # Evaluate accuracy on test set\n",
    "        acc = accuracy(X_test, y_test, weights)\n",
    "        accuracies.append(acc)\n",
    "        \n",
    "    return np.mean(accuracies)\n",
    "\n",
    "# Run K-Fold Cross Validation\n",
    "average_accuracy = k_fold_cross_validation(X, y, k=5)\n",
    "print(f'Average Accuracy: {average_accuracy:.4f}')\n",
    "\n",
    "# KNN\n",
    "class KNN:\n",
    "    def __init__(self):\n",
    "        self.X = None\n",
    "        self.Y = None\n",
    "        self.kvals = [3,5,7,9,11]\n",
    "\n",
    "    def train (self, Xtrain, Ytrain):\n",
    "        self.X = Xtrain\n",
    "        self.Y = Ytrain\n",
    "#         print(sX.shape, Xtrain.shape)\n",
    "        \n",
    "    def test (self, Xtest):\n",
    "        predsperkval = [] \n",
    "        for i in range(Xtest.shape[0]):\n",
    "#             predsperkval.append(get_predictions(Xtest[i], X, Y))\n",
    "            predsperkval.append(self.predict(Xtest[i]))\n",
    "        return predsperkval\n",
    "    \n",
    "    def predict(self, x):\n",
    "        dist = []\n",
    "#         print(X.shape)\n",
    "        for i in range(self.X.shape[0]):\n",
    "            dist.append(np.sqrt(sum((x-X[i])**2)))\n",
    "#             print('test', x, 'trainpt', i,  'dist', np.sqrt(sum((x - i) ** 2)))\n",
    "        sorted_index = np.argsort(dist)\n",
    "        preds = {i:None for i in kvals}\n",
    "#         print(len(dist))\n",
    "        for k in self.kvals:\n",
    "            c = self.Y[[sorted_index[:k]]].flatten()\n",
    "#             print(c)\n",
    "            values, counts = np.unique(c, return_counts=True)\n",
    "\n",
    "            ind = np.argmax(counts)\n",
    "            preds[k] = values[ind]\n",
    "        return preds\n",
    "Xtrain, Ytrain, Xtest, Ytest = split_data(X,Y)\n",
    "# Xtrain.shape\n",
    "kvals = [3,5,7,9,11]\n",
    "kn = KNN()\n",
    "kn.train(Xtrain, Ytrain)\n",
    "# print(Ytest)\n",
    "predictions=kn.test(Xtest)\n",
    "accs = {i:0 for i in kvals}\n",
    "for k in kvals:\n",
    "    for i in range(len(predictions)):\n",
    "        p = predictions[i]\n",
    "        actual = Ytest[i]\n",
    "        predicted = p[k]\n",
    "        if predicted == actual:\n",
    "            accs[k] += 1\n",
    "#             print(Xtest[i], k)\n",
    "    accs[k]  /= len(predictions)\n",
    "print(accs)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "Kvalues, Accuracies = list(accs.keys()),list(accs.values())\n",
    "print(Kvalues, Accuracies)\n",
    "plt.plot(Kvalues, Accuracies,color='green', marker='x', markerfacecolor ='green' , linewidth=2, markersize=10)\n",
    "\n",
    "\n",
    "font1 = {'family':'serif','color':'darkblue','size':20}\n",
    "font2 = {'family':'serif','color':'darkred','size':15}\n",
    "\n",
    "plt.title(\"K values and Accuracies Relationship\", fontdict = font1)\n",
    "plt.xlabel(\"K-Values\", fontdict = font2)\n",
    "plt.ylabel(\"Acuuracies\", fontdict = font2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e0cd66-5abd-4ac2-b896-04fb6ee91591",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
